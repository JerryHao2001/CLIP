{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import Food101, OxfordIIITPet, StanfordCars, Flowers102\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device, download_root=\"./clip\")\n",
    "root = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            features = model.encode_image(images.to(device))\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = CIFAR10(root, download=True, train=True, transform=preprocess)\n",
    "# test = CIFAR10(root, download=True, train=False, transform=preprocess)\n",
    "# # Calculate the image features\n",
    "# train_features, train_labels = get_features(train)\n",
    "# test_features, test_labels = get_features(test)\n",
    "\n",
    "# # Perform logistic regression\n",
    "# classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
    "# classifier.fit(train_features, train_labels)\n",
    "\n",
    "# # Evaluate using the logistic regression classifier\n",
    "# predictions = classifier.predict(test_features)\n",
    "# accuracy = np.mean((test_labels == predictions).astype(float)) * 100.\n",
    "# print(f\"Accuracy = {accuracy:.3f}\")\n",
    "\n",
    "\n",
    "# text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in test.classes]).to(device)\n",
    "\n",
    "# correct=0\n",
    "# # Calculate features\n",
    "# with torch.no_grad():\n",
    "#     text_features = model.encode_text(text_inputs)\n",
    "#     text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "#     for images, labels in tqdm(DataLoader(test, batch_size=1)):\n",
    "#         image_features = model.encode_image(images.to(device))\n",
    "#         image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "#         similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "#         values, indices = similarity[0].topk(1)\n",
    "#         if labels.to('cpu') == indices.to('cpu') : correct+=1\n",
    "        \n",
    "# print(correct/len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CLIP_LR(train,test):\n",
    "\n",
    "    # Calculate the image features\n",
    "    train_features, train_labels = get_features(train)\n",
    "    test_features, test_labels = get_features(test)\n",
    "\n",
    "    # Perform logistic regression\n",
    "    classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
    "    classifier.fit(train_features, train_labels)\n",
    "\n",
    "    # Evaluate using the logistic regression classifier\n",
    "    predictions = classifier.predict(test_features)\n",
    "    accuracy = np.mean((test_labels == predictions).astype(float)) * 100.\n",
    "    print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CLIP_Zero(model, test, classes):\n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classes]).to(device)\n",
    "\n",
    "    correct=0\n",
    "    # Calculate features\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        for images, labels in tqdm(DataLoader(test, batch_size=1)):\n",
    "            image_features = model.encode_image(images.to(device))\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            values, indices = similarity[0].topk(1)\n",
    "            if labels.to('cpu') == indices.to('cpu') : correct+=1\n",
    "            \n",
    "    print(correct/len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VITB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:02<00:00,  8.02it/s]\n",
      "100%|██████████| 100/100 [00:12<00:00,  7.99it/s]\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "e:\\anaconda\\envs\\libfewshot\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   54.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 95.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:53<00:00, 88.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = CIFAR10(root, download=True, train=True, transform=preprocess)\n",
    "test = CIFAR10(root, download=True, train=False, transform=preprocess)\n",
    "run_CLIP_LR(train,test)\n",
    "run_CLIP_Zero(model, test,test.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 733/733 [01:30<00:00,  8.11it/s]\n",
      "100%|██████████| 261/261 [00:33<00:00,  7.68it/s]\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 65.396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\libfewshot\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.5min finished\n"
     ]
    }
   ],
   "source": [
    "train1 = SVHN(root, download=True, split=\"train\", transform=preprocess)\n",
    "test1 = SVHN(root, download=True, split=\"test\", transform=preprocess)\n",
    "run_CLIP_LR(train1,test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "train1 = SVHN(root, download=True, split=\"train\", transform=preprocess)\n",
    "test1 = SVHN(root, download=True, split=\"test\", transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26032 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26032/26032 [15:32<00:00, 27.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11885371850030732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, idx = np.unique(test1.labels, return_index=True)\n",
    "run_CLIP_Zero(model1, test1, test1.labels[np.sort(idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:06<00:00,  1.79it/s]\n",
      "100%|██████████| 62/62 [00:35<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.316\n"
     ]
    }
   ],
   "source": [
    "train = Flowers102(root, download=True, split = 'train', transform=preprocess)\n",
    "test = Flowers102(root, download=True, split = 'test', transform=preprocess)\n",
    "run_CLIP_LR(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Flowers102(root, download=True, split = 'train', transform=preprocess)\n",
    "test = Flowers102(root, download=True, split = 'test', transform=preprocess)\n",
    "classes = [\n",
    "    'pink primrose',\n",
    "    'hard-leaved pocket orchid',\n",
    "    'canterbury bells',\n",
    "    'sweet pea',\n",
    "    'english marigold',\n",
    "    'tiger lily',\n",
    "    'moon orchid',\n",
    "    'bird of paradise',\n",
    "    'monkshood',\n",
    "    'globe thistle',\n",
    "    'snapdragon',\n",
    "    \"colt's foot\",\n",
    "    'king protea',\n",
    "    'spear thistle',\n",
    "    'yellow iris',\n",
    "    'globe flower',\n",
    "    'purple coneflower',\n",
    "    'peruvian lily',\n",
    "    'balloon flower',\n",
    "    'giant white arum lily',\n",
    "    'fire lily',\n",
    "    'pincushion flower',\n",
    "    'fritillary',\n",
    "    'red ginger',\n",
    "    'grape hyacinth',\n",
    "    'corn poppy',\n",
    "    'prince of wales feathers',\n",
    "    'stemless gentian',\n",
    "    'artichoke',\n",
    "    'sweet william',\n",
    "    'carnation',\n",
    "    'garden phlox',\n",
    "    'love in the mist',\n",
    "    'mexican aster',\n",
    "    'alpine sea holly',\n",
    "    'ruby-lipped cattleya',\n",
    "    'cape flower',\n",
    "    'great masterwort',\n",
    "    'siam tulip',\n",
    "    'lenten rose',\n",
    "    'barbeton daisy',\n",
    "    'daffodil',\n",
    "    'sword lily',\n",
    "    'poinsettia',\n",
    "    'bolero deep blue',\n",
    "    'wallflower',\n",
    "    'marigold',\n",
    "    'buttercup',\n",
    "    'oxeye daisy',\n",
    "    'common dandelion',\n",
    "    'petunia',\n",
    "    'wild pansy',\n",
    "    'primula',\n",
    "    'sunflower',\n",
    "    'pelargonium',\n",
    "    'bishop of llandaff',\n",
    "    'gaura',\n",
    "    'geranium',\n",
    "    'orange dahlia',\n",
    "    'pink and yellow dahlia',\n",
    "    'cautleya spicata',\n",
    "    'japanese anemone',\n",
    "    'black-eyed susan',\n",
    "    'silverbush',\n",
    "    'californian poppy',\n",
    "    'osteospermum',\n",
    "    'spring crocus',\n",
    "    'bearded iris',\n",
    "    'windflower',\n",
    "    'tree poppy',\n",
    "    'gazania',\n",
    "    'azalea',\n",
    "    'water lily',\n",
    "    'rose',\n",
    "    'thorn apple',\n",
    "    'morning glory',\n",
    "    'passion flower',\n",
    "    'lotus',\n",
    "    'toad lily',\n",
    "    'anthurium',\n",
    "    'frangipani',\n",
    "    'clematis',\n",
    "    'hibiscus',\n",
    "    'columbine',\n",
    "    'desert-rose',\n",
    "    'tree mallow',\n",
    "    'magnolia',\n",
    "    'cyclamen',\n",
    "    'watercress',\n",
    "    'canna lily',\n",
    "    'hippeastrum',\n",
    "    'bee balm',\n",
    "    'air plant',\n",
    "    'foxglove',\n",
    "    'bougainvillea',\n",
    "    'camellia',\n",
    "    'mallow',\n",
    "    'mexican petunia',\n",
    "    'bromelia',\n",
    "    'blanket flower',\n",
    "    'trumpet creeper',\n",
    "    'blackberry lily',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6149/6149 [01:33<00:00, 65.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6602699625955439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_CLIP_Zero(model, test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:15<00:00, 11085518.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:59<00:00,  8.46it/s]\n",
      "100%|██████████| 100/100 [00:11<00:00,  8.50it/s]\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "e:\\anaconda\\envs\\libfewshot\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 80.030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:42<00:00, 98.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train2 = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
    "test2 = CIFAR100(root, download=True,  train=False, transform=preprocess)\n",
    "run_CLIP_LR(train2,test2)\n",
    "run_CLIP_Zero(model, test2,test2.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:31<00:00,  1.19it/s]\n",
      "100%|██████████| 37/37 [00:31<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 89.207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3669/3669 [00:52<00:00, 70.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8440992095938948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = OxfordIIITPet(root, download=True, transform=preprocess)\n",
    "test = OxfordIIITPet(root, download=True, split = 'test', transform=preprocess)\n",
    "run_CLIP_LR(train,test)\n",
    "run_CLIP_Zero(model, test,test.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1, preprocess1 = clip.load('RN50', device, download_root=\"./clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:02<00:00,  7.95it/s]\n",
      "100%|██████████| 100/100 [00:13<00:00,  7.53it/s]\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "e:\\anaconda\\envs\\libfewshot\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   57.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 95.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:48<00:00, 92.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = CIFAR10(root, download=True, train=True, transform=preprocess1)\n",
    "test = CIFAR10(root, download=True, train=False, transform=preprocess1)\n",
    "run_CLIP_LR(train,test)\n",
    "run_CLIP_Zero(model1,test,test.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 733/733 [01:48<00:00,  6.75it/s]\n",
      "100%|██████████| 261/261 [00:58<00:00,  4.46it/s]\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "e:\\anaconda\\envs\\libfewshot\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 65.396\n"
     ]
    }
   ],
   "source": [
    "train1 = SVHN(root, download=True, split=\"train\", transform=preprocess1)\n",
    "test1 = SVHN(root, download=True, split=\"test\", transform=preprocess1)\n",
    "run_CLIP_LR(train1,test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26032 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26032/26032 [15:32<00:00, 27.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11885371850030732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, idx = np.unique(test1.labels, return_index=True)\n",
    "run_CLIP_Zero(model1, test1, test1.labels[np.sort(idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:19<00:00,  6.26it/s]\n",
      "100%|██████████| 100/100 [00:15<00:00,  6.48it/s]\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "e:\\anaconda\\envs\\libfewshot\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 80.030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:11<00:00, 32.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train2 = CIFAR100(root, download=True, train=True, transform=preprocess1)\n",
    "test2 = CIFAR100(root, download=True,  train=False, transform=preprocess1)\n",
    "run_CLIP_LR(train2,test2)\n",
    "run_CLIP_Zero(model1, test2,test2.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:16<00:00,  2.20it/s]\n",
      "100%|██████████| 37/37 [00:17<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3669/3669 [00:52<00:00, 70.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8315617334423548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train1 = OxfordIIITPet(root, download=True, transform=preprocess1)\n",
    "test1 = OxfordIIITPet(root, download=True, split = 'test', transform=preprocess1)\n",
    "run_CLIP_LR(train1,test1)\n",
    "run_CLIP_Zero(model1, test1,test1.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:06<00:00,  1.81it/s]\n",
      "100%|██████████| 62/62 [00:36<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 82.095\n"
     ]
    }
   ],
   "source": [
    "train1 = Flowers102(root, download=True, split = 'train', transform=preprocess1)\n",
    "test1 = Flowers102(root, download=True, split = 'test', transform=preprocess1)\n",
    "run_CLIP_LR(train1,test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3 = Flowers102(root, download=True, split = 'train', transform=preprocess)\n",
    "test3 = Flowers102(root, download=True, split = 'test', transform=preprocess)\n",
    "classes = [\n",
    "    'pink primrose',\n",
    "    'hard-leaved pocket orchid',\n",
    "    'canterbury bells',\n",
    "    'sweet pea',\n",
    "    'english marigold',\n",
    "    'tiger lily',\n",
    "    'moon orchid',\n",
    "    'bird of paradise',\n",
    "    'monkshood',\n",
    "    'globe thistle',\n",
    "    'snapdragon',\n",
    "    \"colt's foot\",\n",
    "    'king protea',\n",
    "    'spear thistle',\n",
    "    'yellow iris',\n",
    "    'globe flower',\n",
    "    'purple coneflower',\n",
    "    'peruvian lily',\n",
    "    'balloon flower',\n",
    "    'giant white arum lily',\n",
    "    'fire lily',\n",
    "    'pincushion flower',\n",
    "    'fritillary',\n",
    "    'red ginger',\n",
    "    'grape hyacinth',\n",
    "    'corn poppy',\n",
    "    'prince of wales feathers',\n",
    "    'stemless gentian',\n",
    "    'artichoke',\n",
    "    'sweet william',\n",
    "    'carnation',\n",
    "    'garden phlox',\n",
    "    'love in the mist',\n",
    "    'mexican aster',\n",
    "    'alpine sea holly',\n",
    "    'ruby-lipped cattleya',\n",
    "    'cape flower',\n",
    "    'great masterwort',\n",
    "    'siam tulip',\n",
    "    'lenten rose',\n",
    "    'barbeton daisy',\n",
    "    'daffodil',\n",
    "    'sword lily',\n",
    "    'poinsettia',\n",
    "    'bolero deep blue',\n",
    "    'wallflower',\n",
    "    'marigold',\n",
    "    'buttercup',\n",
    "    'oxeye daisy',\n",
    "    'common dandelion',\n",
    "    'petunia',\n",
    "    'wild pansy',\n",
    "    'primula',\n",
    "    'sunflower',\n",
    "    'pelargonium',\n",
    "    'bishop of llandaff',\n",
    "    'gaura',\n",
    "    'geranium',\n",
    "    'orange dahlia',\n",
    "    'pink and yellow dahlia',\n",
    "    'cautleya spicata',\n",
    "    'japanese anemone',\n",
    "    'black-eyed susan',\n",
    "    'silverbush',\n",
    "    'californian poppy',\n",
    "    'osteospermum',\n",
    "    'spring crocus',\n",
    "    'bearded iris',\n",
    "    'windflower',\n",
    "    'tree poppy',\n",
    "    'gazania',\n",
    "    'azalea',\n",
    "    'water lily',\n",
    "    'rose',\n",
    "    'thorn apple',\n",
    "    'morning glory',\n",
    "    'passion flower',\n",
    "    'lotus',\n",
    "    'toad lily',\n",
    "    'anthurium',\n",
    "    'frangipani',\n",
    "    'clematis',\n",
    "    'hibiscus',\n",
    "    'columbine',\n",
    "    'desert-rose',\n",
    "    'tree mallow',\n",
    "    'magnolia',\n",
    "    'cyclamen',\n",
    "    'watercress',\n",
    "    'canna lily',\n",
    "    'hippeastrum',\n",
    "    'bee balm',\n",
    "    'air plant',\n",
    "    'foxglove',\n",
    "    'bougainvillea',\n",
    "    'camellia',\n",
    "    'mallow',\n",
    "    'mexican petunia',\n",
    "    'bromelia',\n",
    "    'blanket flower',\n",
    "    'trumpet creeper',\n",
    "    'blackberry lily',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6149/6149 [01:26<00:00, 71.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6579931696210766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_CLIP_Zero(model1, test3, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESNET 18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Weight Transforms\n",
    "\n",
    "# Initialize model\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "preprocess2 = weights.transforms()\n",
    "model = resnet18(weights=weights)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(512, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train = CIFAR10(root, download=True, train=True, transform=preprocess2)\n",
    "test = CIFAR10(root, download=True, train=False, transform=preprocess2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:45<00:00,  7.39it/s]\n",
      "100%|██████████| 782/782 [01:48<00:00,  7.19it/s]\n",
      "100%|██████████| 782/782 [01:48<00:00,  7.22it/s]\n",
      "100%|██████████| 782/782 [01:47<00:00,  7.27it/s]\n",
      "100%|██████████| 782/782 [01:48<00:00,  7.20it/s]\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for images, target in tqdm(DataLoader(train, 64)):\n",
    "        outputs = model(images.to(device))\n",
    "        loss = criterion(outputs, target.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:21<00:00,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(DataLoader(test, 64)):\n",
    "        outputs = model(inputs.to(device))\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet18_Weights.DEFAULT\n",
    "preprocess2 = weights.transforms()\n",
    "model = resnet18(weights=weights)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(512, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train = CIFAR100(root, download=True, train=True, transform=preprocess2)\n",
    "test = CIFAR100(root, download=True, train=False, transform=preprocess2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:36<00:00,  8.09it/s]\n",
      "100%|██████████| 782/782 [01:36<00:00,  8.09it/s]\n",
      "100%|██████████| 782/782 [01:45<00:00,  7.40it/s]\n",
      "100%|██████████| 782/782 [01:49<00:00,  7.17it/s]\n",
      "100%|██████████| 782/782 [01:49<00:00,  7.16it/s]\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for images, target in tqdm(DataLoader(train, 64)):\n",
    "        outputs = model(images.to(device))\n",
    "        loss = criterion(outputs, target.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:22<00:00,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(DataLoader(test, 64)):\n",
    "        outputs = model(inputs.to(device))\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet18_Weights.DEFAULT\n",
    "preprocess2 = weights.transforms()\n",
    "model = resnet18(weights=weights)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(512, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "train = SVHN(root, download=True, split=\"train\", transform=preprocess2)\n",
    "test = SVHN(root, download=True, split=\"test\", transform=preprocess2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [02:41<00:00,  7.09it/s]\n",
      "100%|██████████| 1145/1145 [02:44<00:00,  6.96it/s]\n",
      "100%|██████████| 1145/1145 [02:41<00:00,  7.10it/s]\n",
      "100%|██████████| 1145/1145 [02:38<00:00,  7.23it/s]\n",
      "100%|██████████| 1145/1145 [02:35<00:00,  7.36it/s]\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for images, target in tqdm(DataLoader(train, 64)):\n",
    "        outputs = model(images.to(device))\n",
    "        loss = criterion(outputs, target.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407/407 [00:54<00:00,  7.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48060079901659497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(DataLoader(test, 64)):\n",
    "        outputs = model(inputs.to(device))\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CLIP_Tip(model, test, classes, cache_img, cache_text, beta=1, alpha=5):\n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classes]).to(device)\n",
    "    cache_img, cache_text = cache_img.to(device), cache_text.to(device)\n",
    "\n",
    "    correct=0\n",
    "    # Calculate features\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        for images, labels in tqdm(DataLoader(test, batch_size=1)):\n",
    "            image_features = model.encode_image(images.to(device))\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            clip_logits  = (100.0 * image_features @ text_features.T)\n",
    "            similarity = clip_logits.softmax(dim=-1)\n",
    "        \n",
    "            affinity = image_features @ cache_img\n",
    "            # print(image_features.shape)\n",
    "            # print(((-1) * (beta - beta * affinity)).exp())\n",
    "            cache_logits = ((-1) * (beta - beta * affinity)).exp() @ cache_text\n",
    "            \n",
    "            tip_logits = clip_logits + cache_logits * alpha\n",
    "            values, indices = tip_logits[0].topk(1)\n",
    "            if labels.to('cpu') == indices.to('cpu') : correct+=1\n",
    "    print(correct/len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cache_model(clip_model, train_loader):\n",
    "    cache_img = []\n",
    "    cache_text = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(1):\n",
    "            train_features = []\n",
    "            for images, target in tqdm(train_loader):\n",
    "                images = images.cuda()\n",
    "                image_features = clip_model.encode_image(images)\n",
    "                train_features.append(image_features)\n",
    "                # print(image_features.shape)\n",
    "                cache_text.append(target)\n",
    "            cache_img.append(torch.cat(train_features, dim=0).unsqueeze(0))\n",
    "    cache_img = torch.cat(cache_img, dim=0).mean(dim=0)\n",
    "    cache_img /= cache_img.norm(dim=-1, keepdim=True)\n",
    "    cache_img = cache_img.permute(1, 0)\n",
    "    cache_text = F.one_hot(torch.cat(cache_text, dim=0)).half()\n",
    "\n",
    "    return cache_img, cache_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:00<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cache_img = []\n",
    "cache_text = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(1):\n",
    "        train_features = []\n",
    "        for images, target in tqdm(DataLoader(train,batch_size=1000)):\n",
    "            images = images.cuda()\n",
    "            image_features = model.encode_image(images)\n",
    "            train_features.append(image_features)\n",
    "            # print(image_features.shape)\n",
    "            cache_text.append(target)\n",
    "        cache_img.append(torch.cat(train_features, dim=0).unsqueeze(0))\n",
    "cache_img = torch.cat(cache_img, dim=0).mean(dim=0)\n",
    "cache_img /= cache_img.norm(dim=-1, keepdim=True)\n",
    "print(cache_img.shape)\n",
    "cache_img = cache_img.permute(1, 0)\n",
    "cache_text = F.one_hot(torch.cat(cache_text, dim=0)).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:05<00:00,  1.30s/it]\n",
      "100%|██████████| 10000/10000 [02:06<00:00, 79.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = CIFAR10(root, download=True, train=True, transform=preprocess)\n",
    "test = CIFAR10(root, download=True, train=False, transform=preprocess)\n",
    "cache_img, cache_text = build_cache_model(model, train)\n",
    "torch.save(cache_img, root + '/keys_' + \"CIFAR10.pt\")\n",
    "torch.save(cache_text, root + '/values_' + \"CIFAR10.pt\")\n",
    "run_CLIP_Tip(model, test, test.classes, cache_img, cache_text, beta=1, alpha=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:02<00:00, 81.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_CLIP_Tip(model, test, test.classes, cache_img, cache_text, beta=1, alpha=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:07<00:00,  1.36s/it]\n",
      "100%|██████████| 10000/10000 [02:06<00:00, 78.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
    "test = CIFAR100(root, download=True,  train=False, transform=preprocess)\n",
    "cache_img, cache_text = build_cache_model(model, train)\n",
    "torch.save(cache_img, root + '/keys_' + \"CIFAR100.pt\")\n",
    "torch.save(cache_text, root + '/values_' + \"CIFAR100.pt\")\n",
    "run_CLIP_Tip(model, test, test.classes, cache_img, cache_text, beta=1, alpha=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:59<00:00, 83.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_CLIP_Tip(model, test, test.classes, cache_img, cache_text, beta=1, alpha=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [01:34<00:00,  1.27s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SVHN' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\JiayiHao\\UR\\Senior\\Fall\\CSC245\\CLIP\\zero_shot_clip.ipynb Cell 40\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/JiayiHao/UR/Senior/Fall/CSC245/CLIP/zero_shot_clip.ipynb#X65sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m torch\u001b[39m.\u001b[39msave(cache_img, root \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/keys_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSVHN.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/JiayiHao/UR/Senior/Fall/CSC245/CLIP/zero_shot_clip.ipynb#X65sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m torch\u001b[39m.\u001b[39msave(cache_text, root \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/values_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSVHN.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/JiayiHao/UR/Senior/Fall/CSC245/CLIP/zero_shot_clip.ipynb#X65sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m run_CLIP_Tip(model, test, test\u001b[39m.\u001b[39;49mclasses, cache_img, cache_text, beta\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SVHN' object has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "train = SVHN(root, download=True, split=\"train\", transform=preprocess)\n",
    "test = SVHN(root, download=True, split=\"test\", transform=preprocess)\n",
    "cache_img, cache_text = build_cache_model(model, train)\n",
    "torch.save(cache_img, root + '/keys_' + \"SVHN.pt\")\n",
    "torch.save(cache_text, root + '/values_' + \"SVHN.pt\")\n",
    "run_CLIP_Tip(model, test, test.classes, cache_img, cache_text, beta=1, alpha=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26032/26032 [05:06<00:00, 85.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1958743085433313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, idx = np.unique(test.labels, return_index=True)\n",
    "run_CLIP_Tip(model, test, test.labels[np.sort(idx)], cache_img, cache_text, beta=1, alpha=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26032/26032 [04:53<00:00, 88.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1958743085433313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_CLIP_Tip(model, test, test.labels[np.sort(idx)], cache_img, cache_text, beta=1, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assuming you have a custom dataset (CustomDataset) inheriting from PyTorch Dataset class\n",
    "# Function to create a custom data loader that loads a specified number of samples per class\n",
    "def create_custom_dataloader(dataset, samples_per_class=10, batch_size=32, shuffle=True):\n",
    "    # Create a dictionary to store indices of each class\n",
    "    class_indices = defaultdict(list)\n",
    "\n",
    "    # Populate class_indices with indices of each class in the dataset\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        class_indices[label].append(idx)\n",
    "\n",
    "    # Select a fixed number of samples from each class\n",
    "    selected_indices = []\n",
    "    for class_idx in class_indices.values():\n",
    "        selected_indices.extend(class_idx[:samples_per_class])\n",
    "\n",
    "    # Create a sampler using the selected indices\n",
    "    sampler = torch.utils.data.sampler.SubsetRandomSampler(selected_indices)\n",
    "\n",
    "    # Create a DataLoader using the created sampler\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# Create a DataLoader that loads 10 samples per class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train = CIFAR10(root, download=True, train=True, transform=preprocess)\n",
    "test = CIFAR10(root, download=True, train=False, transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_Loader = create_custom_dataloader(train, samples_per_class=500, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 22.12it/s]\n"
     ]
    }
   ],
   "source": [
    "cache_img, cache_text = build_cache_model(model, CIFAR10_Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:43<00:00, 96.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_CLIP_Tip(model, test, test.classes, cache_img, cache_text, beta=1, alpha=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples_per_class is 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:13<00:00, 22.51it/s]\n",
      "100%|██████████| 10000/10000 [01:44<00:00, 95.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.871\n",
      "samples_per_class is 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 20.07it/s]\n",
      "100%|██████████| 10000/10000 [01:47<00:00, 92.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8853\n",
      "samples_per_class is 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 15.70it/s]\n",
      "100%|██████████| 10000/10000 [01:53<00:00, 88.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [1000,100,50]:\n",
    "    print(f\"samples_per_class is {i}\")\n",
    "    CIFAR10_Loader = create_custom_dataloader(train, samples_per_class=i, batch_size=32, shuffle=False)\n",
    "    cache_img, cache_text = build_cache_model(model, CIFAR10_Loader)\n",
    "    run_CLIP_Tip(model, test, test.classes, cache_img, cache_text, beta=1, alpha=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples_per_class is 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 12.57it/s]\n",
      "100%|██████████| 10000/10000 [01:42<00:00, 97.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8923\n",
      "samples_per_class is 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  8.12it/s]\n",
      "100%|██████████| 10000/10000 [01:48<00:00, 91.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8973\n",
      "samples_per_class is 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 11.96it/s]\n",
      "100%|██████████| 10000/10000 [01:53<00:00, 87.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [25,10,5]:\n",
    "    print(f\"samples_per_class is {i}\")\n",
    "    CIFAR10_Loader = create_custom_dataloader(train, samples_per_class=i, batch_size=32, shuffle=False)\n",
    "    cache_img, cache_text = build_cache_model(model, CIFAR10_Loader)\n",
    "    run_CLIP_Tip(model, test, test.classes, cache_img, cache_text, beta=1, alpha=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "samples_per_class is 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:12<00:00, 21.44it/s]\n",
      "100%|██████████| 10000/10000 [02:01<00:00, 82.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6142\n",
      "samples_per_class is 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:15<00:00, 19.94it/s]\n",
      "100%|██████████| 10000/10000 [01:51<00:00, 89.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6367\n",
      "samples_per_class is 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.29it/s]\n",
      "100%|██████████| 10000/10000 [01:44<00:00, 95.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6594\n",
      "samples_per_class is 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:03<00:00, 20.07it/s]\n",
      "100%|██████████| 10000/10000 [01:46<00:00, 94.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6649\n",
      "samples_per_class is 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 15.49it/s]\n",
      "100%|██████████| 10000/10000 [01:52<00:00, 89.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
    "test = CIFAR100(root, download=True,  train=False, transform=preprocess)\n",
    "for i in [500,100,50,25,5]:\n",
    "    print(f\"samples_per_class is {i}\")\n",
    "    CIFAR100_Loader = create_custom_dataloader(train, samples_per_class=i, batch_size=32, shuffle=False)\n",
    "    cache_img, cache_text = build_cache_model(model, CIFAR100_Loader)\n",
    "    run_CLIP_Tip(model, test, test.classes, cache_img, cache_text, beta=1, alpha=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n",
      "samples_per_class is 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:13<00:00, 22.79it/s]\n",
      "100%|██████████| 26032/26032 [04:33<00:00, 95.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23528733866011065\n",
      "samples_per_class is 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 20.99it/s]\n",
      "100%|██████████| 26032/26032 [04:38<00:00, 93.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21293023970497849\n",
      "samples_per_class is 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 20.87it/s]\n",
      "100%|██████████| 26032/26032 [04:14<00:00, 102.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15016133988936695\n",
      "samples_per_class is 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 18.49it/s]\n",
      "100%|██████████| 26032/26032 [04:15<00:00, 101.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12150430239704979\n",
      "samples_per_class is 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 14.97it/s]\n",
      "100%|██████████| 26032/26032 [04:18<00:00, 100.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.122579901659496\n",
      "samples_per_class is 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  5.11it/s]\n",
      "100%|██████████| 26032/26032 [04:17<00:00, 101.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10675322679778734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = SVHN(root, download=True, split=\"train\", transform=preprocess)\n",
    "test = SVHN(root, download=True, split=\"test\", transform=preprocess)\n",
    "for i in [1000,500,100,50,25,5]:\n",
    "    print(f\"samples_per_class is {i}\")\n",
    "    SVHN_loader = create_custom_dataloader(train, samples_per_class=i, batch_size=32, shuffle=False)\n",
    "    cache_img, cache_text = build_cache_model(model, SVHN_loader)\n",
    "    _, idx = np.unique(test.labels, return_index=True)\n",
    "    run_CLIP_Tip(model, test, test.labels[np.sort(idx)], cache_img, cache_text, beta=1, alpha=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26032/26032 [05:06<00:00, 85.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1958743085433313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = Flowers102(root, download=True, split = 'train', transform=preprocess)\n",
    "test = Flowers102(root, download=True, split = 'test', transform=preprocess)\n",
    "classes = [\n",
    "    'pink primrose',\n",
    "    'hard-leaved pocket orchid',\n",
    "    'canterbury bells',\n",
    "    'sweet pea',\n",
    "    'english marigold',\n",
    "    'tiger lily',\n",
    "    'moon orchid',\n",
    "    'bird of paradise',\n",
    "    'monkshood',\n",
    "    'globe thistle',\n",
    "    'snapdragon',\n",
    "    \"colt's foot\",\n",
    "    'king protea',\n",
    "    'spear thistle',\n",
    "    'yellow iris',\n",
    "    'globe flower',\n",
    "    'purple coneflower',\n",
    "    'peruvian lily',\n",
    "    'balloon flower',\n",
    "    'giant white arum lily',\n",
    "    'fire lily',\n",
    "    'pincushion flower',\n",
    "    'fritillary',\n",
    "    'red ginger',\n",
    "    'grape hyacinth',\n",
    "    'corn poppy',\n",
    "    'prince of wales feathers',\n",
    "    'stemless gentian',\n",
    "    'artichoke',\n",
    "    'sweet william',\n",
    "    'carnation',\n",
    "    'garden phlox',\n",
    "    'love in the mist',\n",
    "    'mexican aster',\n",
    "    'alpine sea holly',\n",
    "    'ruby-lipped cattleya',\n",
    "    'cape flower',\n",
    "    'great masterwort',\n",
    "    'siam tulip',\n",
    "    'lenten rose',\n",
    "    'barbeton daisy',\n",
    "    'daffodil',\n",
    "    'sword lily',\n",
    "    'poinsettia',\n",
    "    'bolero deep blue',\n",
    "    'wallflower',\n",
    "    'marigold',\n",
    "    'buttercup',\n",
    "    'oxeye daisy',\n",
    "    'common dandelion',\n",
    "    'petunia',\n",
    "    'wild pansy',\n",
    "    'primula',\n",
    "    'sunflower',\n",
    "    'pelargonium',\n",
    "    'bishop of llandaff',\n",
    "    'gaura',\n",
    "    'geranium',\n",
    "    'orange dahlia',\n",
    "    'pink and yellow dahlia',\n",
    "    'cautleya spicata',\n",
    "    'japanese anemone',\n",
    "    'black-eyed susan',\n",
    "    'silverbush',\n",
    "    'californian poppy',\n",
    "    'osteospermum',\n",
    "    'spring crocus',\n",
    "    'bearded iris',\n",
    "    'windflower',\n",
    "    'tree poppy',\n",
    "    'gazania',\n",
    "    'azalea',\n",
    "    'water lily',\n",
    "    'rose',\n",
    "    'thorn apple',\n",
    "    'morning glory',\n",
    "    'passion flower',\n",
    "    'lotus',\n",
    "    'toad lily',\n",
    "    'anthurium',\n",
    "    'frangipani',\n",
    "    'clematis',\n",
    "    'hibiscus',\n",
    "    'columbine',\n",
    "    'desert-rose',\n",
    "    'tree mallow',\n",
    "    'magnolia',\n",
    "    'cyclamen',\n",
    "    'watercress',\n",
    "    'canna lily',\n",
    "    'hippeastrum',\n",
    "    'bee balm',\n",
    "    'air plant',\n",
    "    'foxglove',\n",
    "    'bougainvillea',\n",
    "    'camellia',\n",
    "    'mallow',\n",
    "    'mexican petunia',\n",
    "    'bromelia',\n",
    "    'blanket flower',\n",
    "    'trumpet creeper',\n",
    "    'blackberry lily',\n",
    "]\n",
    "for i in [1000,500,100,50]:\n",
    "    print(f\"samples_per_class is {i}\")\n",
    "    SVHN_loader = create_custom_dataloader(train, samples_per_class=i, batch_size=32, shuffle=False)\n",
    "    cache_img, cache_text = build_cache_model(model, SVHN_loader)\n",
    "\n",
    "    run_CLIP_Tip(model, test, classes, cache_img, cache_text, beta=1, alpha=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libfewshot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
